# Chatbot_Kermit_V2
V1의 용량을 대폭 줄인 Transformer 기반의 챗봇


기존의 챗봇 [커밋V1](https://github.com/ParkEunHyeok/Chatbot_Kermit) 의 경우 GPT-2기반의 Text Generation 모델로 개발했었습니다.   
하지만 GPT 모델의 스케일이 크다보니 학습도 버거워서 제대로 완성하지 못했습니다.   
그래서 다시 Transformer를 공부하면서 딥러닝을 이용한 위키 독스에 올라온 자연어 처리 입문 강의를 참고해서 모델을 다시 구성했습니다.   
데이터셋도 11000개 정도에서 39000개 정도로 늘렸고 용량도 1.4GB정도에서 38MB정도로 줄어든 모습을 볼 수 있습니다.   

made by 박은혁, [신지원](https://github.com/sheenjiwon)
